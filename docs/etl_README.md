# ETL Multi-Agent System - Hexagonal Architecture

Sistema ETL multi-agente construido con **arquitectura hexagonal** (Ports & Adapters) y orquestado con **CrewAI**. Permite procesar datos desde mÃºltiples fuentes (archivos, APIs, bases de datos) y cargarlos en diversos destinos (archivos, data warehouses) con transformaciones, validaciones y autocorrecciÃ³n.

## ğŸ¯ CaracterÃ­sticas

- **Arquitectura Hexagonal**: Dominio desacoplado de frameworks e infraestructura
- **Puertos y Adaptadores**: Interfaces claras para sources, transformations, validations, destinations
- **Casos de Uso Funcionales**: IngestiÃ³n, transformaciÃ³n, validaciÃ³n y carga de datos
- **ValidaciÃ³n Integrada**: Checks de nulls, duplicados y tipos de datos
- **Extensible**: FÃ¡cil adiciÃ³n de nuevos adaptadores sin modificar dominio
- **Preparado para CrewAI** (v0.2+): Arquitectura lista para integraciÃ³n con agentes multi-LLM

## ğŸ“‹ Casos de Uso (Actuales)

1. **TransformaciÃ³n Local**: CSV â†’ CSV con mapeo de columnas y type casting
2. **ValidaciÃ³n de Calidad**: DetecciÃ³n de nulos y duplicados

## ğŸ”® Casos de Uso (Futuros - v0.2+)

1. **MigraciÃ³n de Datos**: CSV â†’ Parquet con limpieza y normalizaciÃ³n
2. **Data Warehousing**: Archivos locales â†’ BigQuery con validaciÃ³n de calidad
3. **IntegraciÃ³n de APIs**: REST API â†’ Base de datos relacional
4. **Data Lakes**: MÃºltiples fuentes â†’ S3/GCS con particionamiento

## ğŸ—ï¸ Arquitectura

```
CLI/Scripts (Driving Adapters)
         â”‚
         â–¼
   Use Cases + Adapters (Application Layer)
         â”‚
         â–¼
Domain (Entities + Use Cases + Ports)
         â”‚
         â–¼
Adapters (Files, DBs, Cloud, Driven)
```

**Nota**: La implementaciÃ³n actual utiliza casos de uso directamente. CrewAI Crew y Flow estÃ¡n definidos pero son opcionales para futuras extensiones.

Ver [Diagrama Detallado](../docs/etl_architecture.md)

## ğŸš€ InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/lraigosov/multi-agent.git
cd multi-agent

# Activar entorno virtual
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
# o con Poetry
poetry install
```

## âš™ï¸ ConfiguraciÃ³n

Copia `.env.example` y configura:

```env
# LLM Provider (openai, gemini, anthropic)
ETL_LLM_PROVIDER=gemini
ETL_LLM_MODEL=gemini-2.0-flash
ETL_LLM_TEMPERATURE=0.1

# Google Gemini (gratuito)
GOOGLE_API_KEY=your_key_here

# OpenAI (alternativo)
OPENAI_API_KEY=sk-your_key_here

# ConfiguraciÃ³n ETL
ETL_ENABLE_QUALITY_CHECKS=true
ETL_FAIL_ON_VALIDATION_ERRORS=false
ETL_LOG_LEVEL=INFO
```

## ğŸ® Uso

### Demo Funcional

```bash
# Demo con caso de Ã©xito y manejo de errores
python examples/demo_etl.py
```

**Salida esperada:**
- Ingesta de datos desde CSV
- TransformaciÃ³n de columnas (mapeo y type casting)
- ValidaciÃ³n de calidad de datos
- Carga a archivo CSV destino en `outputs/`

**Nota**: La demo ejecuta el pipeline ETL completo sin dependencias de LLMs ni CrewAI Crew/Flow.
```

### Uso ProgramÃ¡tico

```python
from etl_multiagent.flows.etl_pipeline_flow import ETLPipelineFlow, ETLFlowState

# Configurar estado del flow
state = ETLFlowState(
    source_uri="data/sales.csv",
    source_format="csv",
    dest_uri="outputs/sales_clean.parquet",
    dest_format="parquet",
    mappings={
        "sale_id": "id",
        "customer_name": "name",
        "amount": "sale_amount",
    },
    target_schema={
        "sale_id": "int64",
        "customer_name": "object",
        "sale_amount": "float64",
    },
)

# Ejecutar pipeline
flow = ETLPipelineFlow()
flow.state = state
result = flow.kickoff()

# Revisar resultados
print(f"Status: {state.validation_report['status']}")
print(f"Output: {state.load_result['path']}")
```

## ğŸ“š DocumentaciÃ³n

- [Arquitectura Hexagonal](../docs/etl_architecture.md): Diagrama y principios
- [DiseÃ±o de Agentes](../docs/etl_agents_design.md): Roles, goals, herramientas, LLMs
- [Riesgos y Mitigaciones](../docs/etl_risks_mitigations.md): Limitaciones y planes futuros
- [Checklist ProducciÃ³n](../docs/etl_production_checklist.md): Pasos para hardening

## ğŸ§ª Testing

```bash
# Tests unitarios
pytest tests/test_etl_domain.py

# Tests de integraciÃ³n
pytest tests/test_etl_adapters.py

# Demo completo
python examples/demo_etl.py
```

## ğŸ“¦ Estructura del Proyecto

```
src/etl_multiagent/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities.py         # DataSource, DataBatch, TransformationJob, DataDestination
â”‚   â””â”€â”€ use_cases.py        # IngestData, TransformData, LoadData, ReconcileJobResult
â”œâ”€â”€ ports/
â”‚   â””â”€â”€ __init__.py         # SourcePort, TransformPort, ValidationPort, DestinationPort
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ sources.py          # FileSourceAdapter, S3SourceAdapter (stub)
â”‚   â”œâ”€â”€ destinations.py     # FileDestinationAdapter, BigQueryDestinationAdapter (stub)
â”‚   â””â”€â”€ transformers.py     # PandasTransformAdapter, ValidationAdapter
â”œâ”€â”€ crews/
â”‚   â””â”€â”€ etl_orchestration_crew.py  # 5 agentes especializados
â”œâ”€â”€ flows/
â”‚   â””â”€â”€ etl_pipeline_flow.py       # Flow con listen-based orchestration
â””â”€â”€ config/
    â””â”€â”€ settings.py         # ETLSettings (pydantic)

config/
â”œâ”€â”€ etl_agents.yaml         # ConfiguraciÃ³n de agentes
â”œâ”€â”€ etl_tasks.yaml          # ConfiguraciÃ³n de tareas
â””â”€â”€ etl_config.yaml         # ConfiguraciÃ³n de herramientas
```

## ğŸ”§ Extensibilidad

### Agregar Nueva Fuente

1. Implementar adaptador:
```python
class BigQuerySourceAdapter:
    def read(self, source: DataSource) -> DataBatch:
        # Implementar query a BigQuery
        ...
```

2. Registrar en use case:
```python
if source.kind == "bigquery":
    adapter = BigQuerySourceAdapter()
```

### Agregar Nuevo Destino

1. Implementar adaptador:
```python
class SnowflakeDestinationAdapter:
    def write(self, batch: DataBatch, dest: DataDestination) -> dict:
        # Implementar carga a Snowflake
        ...
```

2. Usar en flow o crew sin modificar dominio.

## ğŸ¤ ContribuciÃ³n

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m "Add nueva funcionalidad"`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

## ğŸ“„ Licencia

MIT License - ver [LICENSE](../LICENSE) para detalles.

## ğŸ™ Agradecimientos

- [CrewAI](https://github.com/crewAIInc/crewAI) por el framework multi-agente
- [Alistair Cockburn](https://alistair.cockburn.us/hexagonal-architecture/) por Hexagonal Architecture
- Comunidad open source por herramientas y feedback
