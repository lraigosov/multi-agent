# ETL Hexagonal (Ports & Adapters, sin LLMs)

Pipeline ETL determinista construido con **arquitectura hexagonal** (Ports & Adapters). Usa pandas para ingesta/transformaciÃ³n/validaciÃ³n/carga y puede orquestarse con el `ETLPipelineFlow` (CrewAI Flow) incluido. No requiere LLMs para ejecutarse.

## ğŸ¯ CaracterÃ­sticas

- **Arquitectura Hexagonal**: Dominio desacoplado de frameworks e infraestructura
- **Puertos y Adaptadores**: Interfaces claras para sources, transformations, validations, destinations
- **Casos de Uso Funcionales**: IngestiÃ³n, transformaciÃ³n, validaciÃ³n y carga de datos
- **ValidaciÃ³n Integrada**: Checks de nulls y duplicados
- **Extensible**: FÃ¡cil adiciÃ³n de nuevos adaptadores sin modificar dominio
- **Flow incluido**: `ETLPipelineFlow` orquesta los casos de uso con los adaptadores actuales

## ğŸ“‹ Casos de Uso actuales

1. **TransformaciÃ³n Local**: CSV â†’ CSV/Parquet/Excel con mapeo de columnas y type casting
2. **ValidaciÃ³n de Calidad**: DetecciÃ³n de nulos y duplicados

## ğŸ—ï¸ Arquitectura

```
CLI/Scripts (Driving Adapters)
         â”‚
         â–¼
   Use Cases + Adapters (Application Layer)
         â”‚
         â–¼
Domain (Entities + Use Cases + Ports)
         â”‚
         â–¼
Adapters (Files, DBs, Cloud, Driven)
```

**Nota**: La demo usa los casos de uso directamente. El Flow `ETLPipelineFlow` estÃ¡ disponible si quieres orquestaciÃ³n declarativa.

Ver [etl_architecture.md](etl_architecture.md) para el diagrama.

## ğŸš€ InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/lraigosov/multi-agent.git
cd multi-agent

# Activar entorno virtual
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
# o con Poetry
poetry install
```

## ğŸ® Uso

### Demo Funcional

```bash
# Demo con caso de Ã©xito y manejo de errores
python examples/demo_etl.py
```

**Salida esperada:**
- Ingesta de datos desde CSV
- TransformaciÃ³n de columnas (mapeo y type casting)
- ValidaciÃ³n de calidad de datos
- Carga a archivo CSV destino en `outputs/`

**Nota**: La demo ejecuta el pipeline ETL completo sin dependencias de LLMs ni CrewAI Crew/Flow.

### Uso ProgramÃ¡tico

```python
from etl_multiagent.flows.etl_pipeline_flow import ETLPipelineFlow, ETLFlowState

# Configurar estado del flow
state = ETLFlowState(
    source_uri="data/sales.csv",
    source_format="csv",
    dest_uri="outputs/sales_clean.parquet",
    dest_format="parquet",
    mappings={
        "sale_id": "id",
        "customer_name": "name",
        "amount": "sale_amount",
    },
    target_schema={
        "sale_id": "int64",
        "customer_name": "object",
        "sale_amount": "float64",
    },
)

# Ejecutar pipeline
flow = ETLPipelineFlow()
flow.state = state
result = flow.kickoff()

# Revisar resultados
print(f"Status: {state.validation_report['status']}")
print(f"Output: {state.load_result['path']}")
```

## ğŸ“š DocumentaciÃ³n

- [etl_architecture.md](etl_architecture.md): Diagrama y principios aplicados

## ğŸ“¦ Estructura del Proyecto

```
src/etl_multiagent/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities.py         # DataSource, DataBatch, TransformationJob, DataDestination
â”‚   â””â”€â”€ use_cases.py        # IngestData, TransformData, LoadData, ReconcileJobResult
â”œâ”€â”€ ports/
â”‚   â””â”€â”€ __init__.py         # SourcePort, TransformPort, ValidationPort, DestinationPort
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ sources.py          # FileSourceAdapter, S3SourceAdapter (stub)
â”‚   â”œâ”€â”€ destinations.py     # FileDestinationAdapter, BigQueryDestinationAdapter (stub)
â”‚   â””â”€â”€ transformers.py     # PandasTransformAdapter, ValidationAdapter
â”œâ”€â”€ crews/
â”‚   â””â”€â”€ etl_orchestration_crew.py  # 5 agentes especializados
â”œâ”€â”€ flows/
â”‚   â””â”€â”€ etl_pipeline_flow.py       # Flow con listen-based orchestration
â””â”€â”€ config/
    â””â”€â”€ settings.py         # ETLSettings (pydantic)

config/
â”œâ”€â”€ etl_agents.yaml         # ConfiguraciÃ³n de agentes
â”œâ”€â”€ etl_tasks.yaml          # ConfiguraciÃ³n de tareas
â””â”€â”€ etl_config.yaml         # ConfiguraciÃ³n de herramientas
```

## ğŸ”§ Extensibilidad

### Agregar Nueva Fuente

1. Implementar adaptador:
```python
class BigQuerySourceAdapter:
    def read(self, source: DataSource) -> DataBatch:
        # Implementar query a BigQuery
        ...
```

2. Registrar en use case:
```python
if source.kind == "bigquery":
    adapter = BigQuerySourceAdapter()
```

### Agregar Nuevo Destino

1. Implementar adaptador:
```python
class SnowflakeDestinationAdapter:
    def write(self, batch: DataBatch, dest: DataDestination) -> dict:
        # Implementar carga a Snowflake
        ...
```

2. Usar en flow o crew sin modificar dominio.

## ğŸ¤ ContribuciÃ³n

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m "Add nueva funcionalidad"`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

## ğŸ“„ Licencia

MIT License - ver [LICENSE](../LICENSE) para detalles.

## ğŸ™ Agradecimientos

- [CrewAI](https://github.com/crewAIInc/crewAI) por el framework multi-agente
- [Alistair Cockburn](https://alistair.cockburn.us/hexagonal-architecture/) por Hexagonal Architecture
- Comunidad open source por herramientas y feedback
